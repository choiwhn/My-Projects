{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6413767d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6413767d",
    "outputId": "7de52215-536e-4688-c280-443695a966b2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# One-hot encoding\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a61dd84",
   "metadata": {
    "id": "8a61dd84"
   },
   "outputs": [],
   "source": [
    "def zlst(m):\n",
    "    zlst=np.zeros((2**m,m))\n",
    "    for i in range(2**m):\n",
    "        z=format(i, 'b').zfill(m)\n",
    "        z=np.array(list(z))\n",
    "        zlst[i,:]=z\n",
    "    return zlst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7deca37e",
   "metadata": {
    "id": "7deca37e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z / np.sum(exp_z)\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, lr): #p,m,g\n",
    "        self.p = input_size #p\n",
    "        self.m = hidden_size #m\n",
    "        self.g = output_size #g\n",
    "        self.lr = lr\n",
    "        self.zlst=zlst(self.m)\n",
    "\n",
    "        # weight initialize & shape construction\n",
    "        np.random.seed(1183)\n",
    "        self.W = np.random.randn(self.p, self.m) #p*m\n",
    "        np.random.seed(202)\n",
    "        self.V = np.random.randn(self.m, self.g) #m*g\n",
    "\n",
    "    def forward(self, X): #forward propagation\n",
    "\n",
    "        self.A1 = self.W.T @ X #m*1\n",
    "        self.U = sigmoid(self.A1) #m*1\n",
    "\n",
    "        self.A2 = self.V.T @ self.U #g*1\n",
    "        self.O = softmax(self.A2) #g*1\n",
    "        return self.O\n",
    "\n",
    "########################################################################\n",
    "\n",
    "    def M_step(self, X, y): #EM algorithm\n",
    "\n",
    "        grad_W = np.zeros((self.p, self.m)) #p*m\n",
    "        grad_V = np.zeros((self.m, self.g)) #m*g\n",
    "\n",
    "        ####################\n",
    "\n",
    "        for h in range(self.m):\n",
    "            grad_Wh=0\n",
    "            self.zlsth=self.zlst[self.zlst[:,h]==1]\n",
    "            for j in range(len(X)):\n",
    "                sumz, sumzh=self.E_step_W(j,X,y)\n",
    "                grad_Wh += (sumzh/sumz-self.U[h])*X[j,:]\n",
    "\n",
    "            grad_W[:,h]=grad_Wh\n",
    "\n",
    "        ####################\n",
    "\n",
    "        for h in range(self.m):\n",
    "            self.zlsth=self.zlst[self.zlst[:,h]==1]\n",
    "            for i in range(self.g):\n",
    "                for j in range(len(X)):\n",
    "                    sumz, sumzy = self.E_step_V(j, X, y, i)\n",
    "                    grad_V[h,i] += sumzy/sumz\n",
    "\n",
    "        ####################\n",
    "\n",
    "        # update weight & bias\n",
    "        self.W += grad_W * self.lr\n",
    "        self.V += grad_V * self.lr\n",
    "\n",
    "########################################################################\n",
    "\n",
    "    def E_step_W(self, j, X, y):\n",
    "        self.forward(X[j,:])\n",
    "\n",
    "        sumz=0\n",
    "        for z in self.zlst:\n",
    "            pr_xyz=1\n",
    "            for h in range(self.m):\n",
    "                pr_xyz*=self.U[h]**z[h]*(1-self.U[h])**(1-z[h])\n",
    "            for i in range(self.g):\n",
    "                pr_xyz*=softmax(self.V.T @ z)[i]**y[j,i]\n",
    "            sumz+=pr_xyz\n",
    "\n",
    "        sumzh=0\n",
    "        for z in self.zlsth:\n",
    "            pr_xyz=1\n",
    "            for h in range(self.m):\n",
    "                pr_xyz*=self.U[h]**z[h]*(1-self.U[h])**(1-z[h])\n",
    "            for i in range(self.g):\n",
    "                pr_xyz*=softmax(self.V.T @ z)[i]**y[j,i]\n",
    "            sumzh+=pr_xyz\n",
    "\n",
    "        return sumz, sumzh\n",
    "\n",
    "    def E_step_V(self, j, X, y, i):\n",
    "        self.forward(X[j,:])\n",
    "\n",
    "        sumz=0\n",
    "        for z in self.zlst:\n",
    "            pr_xyz=1\n",
    "            for h in range(self.m):\n",
    "                pr_xyz*=self.U[h]**z[h]*(1-self.U[h])**(1-z[h])\n",
    "            for i in range(self.g):\n",
    "                pr_xyz*=softmax(self.V.T @ z)[i]**y[j,i]\n",
    "            sumz+=pr_xyz\n",
    "\n",
    "        sumzy=0\n",
    "        for z in self.zlsth:\n",
    "            pr_xyz=1\n",
    "            for h_ in range(self.m):\n",
    "                pr_xyz*=self.U[h_]**z[h_]*(1-self.U[h_])**(1-z[h_])\n",
    "            for i_ in range(self.g):\n",
    "                pr_xyz*=softmax(self.V.T @ z)[i_]**y[j,i_]\n",
    "            pr_xyz*=y[j,i]-softmax(self.V.T @ z)[i]\n",
    "            sumzy+=pr_xyz\n",
    "\n",
    "        return sumz, sumzy\n",
    "\n",
    "\n",
    "\n",
    "    def Train(self, X, y, epochs):\n",
    "        loss_set=list()\n",
    "        for n in range(len(X)):\n",
    "            y_pred = self.forward(X[n,:])\n",
    "            loss = -np.sum(y[n,:]*np.log(y_pred))\n",
    "            loss_set.append(loss)\n",
    "        avgloss = np.mean(loss_set)\n",
    "        print(f'Epoch {0}, Loss: {avgloss}')\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            self.M_step(X, y)\n",
    "\n",
    "            losses=list()\n",
    "            for n in range(len(X)):\n",
    "                y_pred = self.forward(X[n,:])\n",
    "                loss = -np.sum(y[n,:]*np.log(y_pred)) #Cross Entropy Loss\n",
    "                losses.append(loss)\n",
    "            avgloss=np.mean(losses)\n",
    "\n",
    "            if (epoch+1) % 5 == 0: print(f'Epoch {epoch+1}, Loss: {avgloss}')\n",
    "\n",
    "    def Test(self, X):\n",
    "        testoutput=[]\n",
    "        for n in range(len(X)):\n",
    "            y_pred = self.forward(X[n,:])\n",
    "            testoutput.append(np.argmax(y_pred))\n",
    "        return testoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a415a7c4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a415a7c4",
    "outputId": "4912f83d-450b-4579-ddfa-ecb0621ebaba",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.1427729846642725\n",
      "Epoch 5, Loss: 0.8477192072820486\n",
      "Epoch 10, Loss: 0.8088932560503146\n",
      "Epoch 15, Loss: 0.8013409928099052\n",
      "Epoch 20, Loss: 0.7953260351338284\n",
      "Epoch 25, Loss: 0.7862787851641184\n",
      "Epoch 30, Loss: 0.6765491949913677\n",
      "Epoch 35, Loss: 0.6733092653604303\n",
      "Epoch 40, Loss: 0.670328548725725\n",
      "Epoch 45, Loss: 0.6676518890360496\n",
      "Epoch 50, Loss: 0.6652140857718162\n"
     ]
    }
   ],
   "source": [
    "#setting hyperparameters\n",
    "epochs=50\n",
    "lr=0.005\n",
    "\n",
    "NN=NeuralNetwork(4,7,3,lr)\n",
    "NN.Train(X_train,y_train,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0691967",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0691967",
    "outputId": "3d7538c3-3a57-4ac9-83db-8db96065f1ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.33%\n"
     ]
    }
   ],
   "source": [
    "testoutput = NN.Test(X_test)\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "#accuracy\n",
    "accuracy = np.mean(testoutput == y_test_labels)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97ed3e0b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "97ed3e0b",
    "outputId": "f1c28b58-72d3-45bc-fac7-e6ba8c7ac9de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.936249   0.04692205 0.01682896] [1. 0. 0.] True\n",
      "[0.27988311 0.32700521 0.39311167] [0. 0. 1.] True\n",
      "[0.28207146 0.33332921 0.38459933] [0. 0. 1.] True\n",
      "[0.95591958 0.03276875 0.01131167] [1. 0. 0.] True\n",
      "[0.27057917 0.31213924 0.4172816 ] [0. 0. 1.] True\n",
      "[0.33065804 0.38445328 0.28488868] [0. 1. 0.] True\n",
      "[0.95479909 0.03355867 0.01164225] [1. 0. 0.] True\n",
      "[0.27467353 0.31926968 0.4060568 ] [0. 0. 1.] True\n",
      "[0.94697415 0.03924066 0.01378519] [1. 0. 0.] True\n",
      "[0.31662273 0.37827904 0.30509823] [0. 1. 0.] True\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    pred=NN.forward(X_test[i,:])\n",
    "    target=y_test[i,:]\n",
    "    print(pred,target,np.argmax(pred)==np.argmax(target))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
