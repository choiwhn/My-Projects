{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6413767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Iris 데이터셋 로드\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# One-hot 인코딩\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# 데이터셋을 학습용과 테스트용으로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1017) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7deca37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "def sigmoid_deriv(z):\n",
    "        return sigmoid(z) * (1 - sigmoid(z))\n",
    "    \n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z / np.sum(exp_z)\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate): #p,q,r\n",
    "        self.input_size = input_size #p\n",
    "        self.hidden_size = hidden_size #q\n",
    "        self.output_size = output_size #r\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # weight initialize & shape construction\n",
    "        np.random.seed(202)\n",
    "        self.W1 = np.random.randn(self.input_size, self.hidden_size) #p*q\n",
    "        np.random.seed(107)\n",
    "        self.b1 = np.zeros((self.hidden_size)) #q*1\n",
    "        np.random.seed(1138)\n",
    "        self.W2 = np.random.randn(self.hidden_size, self.output_size) #q*r\n",
    "        np.random.seed(27)\n",
    "        self.b2 = np.zeros((self.output_size)) #r*1\n",
    "    \n",
    "    def forward(self, X): #forward propagation\n",
    "        self.Z1 = X @ self.W1 + self.b1 #q*1\n",
    "        self.A1 = sigmoid(self.Z1) #q*1\n",
    "        self.Z2 = self.A1 @ self.W2 + self.b2 #r*1\n",
    "        self.A2 = softmax(self.Z2) #r*1\n",
    "        return self.A2\n",
    "\n",
    "########################################################################\n",
    "    def backward(self, X, y, y_pred): #backpropagation\n",
    "        L_Z2 = (y - y_pred)*(-1)\n",
    "        A1_Z1 = sigmoid_deriv(self.A1) #q*1\n",
    "        \n",
    "        grad_W2 = np.zeros((self.hidden_size,self.output_size)) #q*r\n",
    "        for q in range(self.hidden_size):\n",
    "            for r in range(self.output_size):\n",
    "                grad_W2[q,r]=L_Z2[r]*self.A1[q]\n",
    "        grad_b2 = np.zeros(self.output_size) #r*1\n",
    "        for r in range(self.output_size):\n",
    "            grad_b2[r]=L_Z2[r]*1\n",
    "\n",
    "        grad_W1 = np.zeros((self.input_size,self.hidden_size)) #p*q\n",
    "        for p in range(self.input_size):\n",
    "            for q in range(self.hidden_size):\n",
    "                for r in range(self.output_size):\n",
    "                    grad_W1[p,q]+=L_Z2[r]*self.W2[q,r]*A1_Z1[q]*X[p]\n",
    "        grad_b1 = np.zeros(self.hidden_size) #q*1\n",
    "        for p in range(self.input_size):\n",
    "            for q in range(self.hidden_size):\n",
    "                for r in range(self.output_size):\n",
    "                    grad_b1[q]+=L_Z2[r]*self.W2[q,r]*A1_Z1[q]*1        \n",
    "\n",
    "        # update weight & bias\n",
    "        self.W2 -= grad_W2 * self.learning_rate\n",
    "        self.b2 -= grad_b2 * self.learning_rate\n",
    "        self.W1 -= grad_W1 * self.learning_rate\n",
    "        self.b1 -= grad_b1 * self.learning_rate\n",
    "        \n",
    "########################################################################\n",
    "        \n",
    "    def Train(self, X, y, epochs):\n",
    "        loss_set=list()\n",
    "        for n in range(len(X)):\n",
    "            y_pred = self.forward(X[n,:])\n",
    "            loss = -np.sum(y[n,:]*np.log(y_pred))\n",
    "            loss_set.append(loss)\n",
    "        avgloss = np.mean(loss_set)\n",
    "        print(f'Epoch {0}, Loss: {avgloss}')        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            loss_set=list()\n",
    "            for n in range(len(X)):\n",
    "                y_pred = self.forward(X[n,:])\n",
    "                loss = -np.sum(y[n,:]*np.log(y_pred)) #Cross Entropy Loss\n",
    "                self.backward(X[n,:], y[n,:], y_pred) \n",
    "                loss_set.append(loss)\n",
    "            avgloss = np.mean(loss_set)\n",
    "            \n",
    "            if (epoch+1) % 5 == 0: print(f'Epoch {epoch+1}, Loss: {avgloss}')\n",
    "           \n",
    "    def Test(self, X):\n",
    "        testoutput=[]\n",
    "        for n in range(len(X)):\n",
    "            y_pred = self.forward(X[n,:])\n",
    "            testoutput.append(np.argmax(y_pred))    \n",
    "        return testoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60e600a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.1799459307487936\n",
      "Epoch 5, Loss: 0.7697343605798745\n",
      "Epoch 10, Loss: 0.6724774620576618\n",
      "Epoch 15, Loss: 0.5865647960528668\n",
      "Epoch 20, Loss: 0.5445126310389149\n",
      "Epoch 25, Loss: 0.5078813746853241\n",
      "Epoch 30, Loss: 0.46463978978466247\n"
     ]
    }
   ],
   "source": [
    "#setting hyperparameters\n",
    "learning_rate=0.005\n",
    "epochs=30\n",
    "\n",
    "NN=NeuralNetwork(4,7,3,learning_rate)\n",
    "NN.Train(X_train,y_train,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "525efe1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.67%\n"
     ]
    }
   ],
   "source": [
    "testoutput = NN.Test(X_test)\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "#accuracy\n",
    "accuracy = np.mean(testoutput == y_test_labels)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b98af3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.79087513 0.17280196 0.03632291] [1. 0. 0.] True\n",
      "[0.0147225  0.34725775 0.63801976] [0. 0. 1.] True\n",
      "[0.01470303 0.33480121 0.65049575] [0. 0. 1.] True\n",
      "[0.83338793 0.13828392 0.02832815] [1. 0. 0.] True\n",
      "[0.00902271 0.33238056 0.65859673] [0. 0. 1.] True\n",
      "[0.17068413 0.52608564 0.30323023] [0. 1. 0.] True\n",
      "[0.8299676  0.14205715 0.02797525] [1. 0. 0.] True\n",
      "[0.01058652 0.32585278 0.6635607 ] [0. 0. 1.] True\n",
      "[0.81611671 0.15257764 0.03130565] [1. 0. 0.] True\n",
      "[0.13539729 0.5567226  0.30788011] [0. 1. 0.] True\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    pred=NN.forward(X_test[i,:])\n",
    "    target=y_test[i,:]\n",
    "    print(pred,target,np.argmax(pred)==np.argmax(target))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
